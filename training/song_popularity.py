# -*- coding: utf-8 -*-
"""song_popularity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FOJh20O2DcemNHO16tVYjlT3q_V4IX-l
"""

import pandas as pd

df = pd.read_csv('/content/dataset.csv')
print("Data shape: ", df.shape)
df.head()

"""# Preprocessing

Check the data type and missing values
"""

display(df.info())
display(df.isnull().sum())

"""Remove the missing value rows"""

df.drop(df[df['track_genre'].isnull()].index, inplace=True)
df.drop(df[df['track_name'].isnull()].index, inplace=True)
df.drop(df[df['album_name'].isnull()].index, inplace=True)
df.drop(df[df['artists'].isnull()].index, inplace=True)

print(df.isnull().sum())

"""Remove index value column"""

df.drop('Unnamed: 0', axis=1, inplace=True)
df.head()

"""Do the ANOVA test to check mean popularity differ significantly across artists, album_name, track_name track_genre"""

from scipy.stats import f_oneway

# Select top N artists only for testing
top_artists = df['artists'].value_counts().index[:500]
df_top = df[df['artists'].isin(top_artists)]

groups = [df_top[df_top['artists']==a]['popularity'] for a in top_artists]
f_stat, p_val = f_oneway(*groups)
print(f"F-statistic: {f_stat}, p-value: {p_val}")

from scipy.stats import f_oneway

# Select top N album only for testing
top_album_name = df['album_name'].value_counts().index[:100]
df_top = df[df['artists'].isin(top_artists)]

groups = [df_top[df_top['album_name']==a]['popularity'] for a in top_album_name]
f_stat, p_val = f_oneway(*groups)
print(f"F-statistic: {f_stat}, p-value: {p_val}")

# Select top N track_name only for testing
top_track_name = df['track_name'].value_counts().index[:50]
df_top = df[df['artists'].isin(top_artists)]

groups = [df_top[df_top['track_name']==a]['popularity'] for a in top_track_name]
f_stat, p_val = f_oneway(*groups)
print(f"F-statistic: {f_stat}, p-value: {p_val}")

# Select top N track_genre only for testing
top_track_genre = df['track_genre'].value_counts().index[:50]
df_top = df[df['artists'].isin(top_artists)]

groups = [df_top[df_top['track_genre']==a]['popularity'] for a in top_track_genre]
f_stat, p_val = f_oneway(*groups)
print(f"F-statistic: {f_stat}, p-value: {p_val}")

df['artists'].nunique() / df.shape[0] * 100

df['album_name'].nunique() / df.shape[0] * 100

df['track_genre'].nunique() / df.shape[0] * 100

df.drop('artists', axis=1, inplace=True)
df.drop('track_name', axis=1, inplace=True)
df.drop('album_name', axis=1, inplace=True)
df.drop('track_id', axis=1, inplace=True)

"""Check duplicate rows"""

print(df.duplicated().sum())
dupe_rows = df[df.duplicated(keep=False)]
dupe_rows = dupe_rows.sort_values(list(df.columns))
dupe_rows

df.drop_duplicates(inplace=True)
print(df.duplicated().sum())

df.info()

"""# Exploratory Data Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

numerical_cols = ['popularity', 'duration_ms', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 4, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

"""# Correlation analysis

"""

numerical_df = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numerical_df.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""Remove energy column that highly corelated with loudness"""

df.drop('energy', axis=1, inplace=True)

"""Now the dataset"""

df.head()

"""Encoding the explicit"""

df['explicit'] = df['explicit'].astype(int)
display(df.head())

"""Encoding the track genre"""

df['track_genre'].unique()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
int_encoded = le.fit_transform(df['track_genre'])
df['track_genre'] = int_encoded

df.head()

"""Normalize the values"""

from sklearn.preprocessing import StandardScaler

SCALE_COLS = [
    "duration_ms", "danceability", "loudness", "speechiness",
    "acousticness", "instrumentalness", "liveness", "valence", "tempo"
]

scaler = StandardScaler()
df[SCALE_COLS] = scaler.fit_transform(df[SCALE_COLS])

df.head()

"""Split features"""

X = df.drop('popularity', axis=1)
y = df['popularity']

"""Train/test split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Check where with model is good for that"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "Neural Network": MLPRegressor(hidden_layer_sizes=(64,32), max_iter=200, random_state=42)
}

# --- Train and evaluate ---
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)

    results.append([name, rmse, mae, r2])

# --- Display comparison ---
results_df = pd.DataFrame(results, columns=["Model", "RMSE", "MAE", "R2"])
print(results_df)

"""According to output:
Lowest RMSE (16.3) → Predictions are closer to true popularity on average.

Lowest MAE (11.6) → Smallest average error size.

Highest R² (0.42) → Explains ~42% of the variability in popularity (other models explain far less).
"""

import matplotlib.pyplot as plt
import numpy as np

def bar_with_value_labels(ax, labels, values, rotation=25, fmt="{:.3f}"):
    x = np.arange(len(labels))
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=rotation, ha="right")
    for b, v in zip(bars, values):
        ax.text(b.get_x() + b.get_width()/2, b.get_height(), fmt.format(v),
                ha="center", va="bottom")


# --- RMSE (lower is better) ---
rmse_df = results_df.sort_values("RMSE")
fig, ax = plt.subplots()
bar_with_value_labels(ax, rmse_df["Model"].tolist(), rmse_df["RMSE"].to_numpy())
ax.set_title("Model Comparison — RMSE (Root Mean Squared Error): Lower is better")
ax.set_xlabel("Models")
ax.set_ylabel("RMSE")
fig.tight_layout()
plt.show()


# --- MAE (lower is better) ---
mae_df = results_df.sort_values("MAE")
fig, ax = plt.subplots()
bar_with_value_labels(ax, mae_df["Model"].tolist(), mae_df["MAE"].to_numpy())
ax.set_title("Model Comparison — MAE (Mean Absolute Error): Lower is better")
ax.set_xlabel("Models")
ax.set_ylabel("MAE")
fig.tight_layout()
plt.show()


# --- R² (higher is better) ---
r2_df = results_df.sort_values("R2", ascending=False)
fig, ax = plt.subplots()
bar_with_value_labels(ax, r2_df["Model"].tolist(), r2_df["R2"].to_numpy())
ax.set_title("Model Comparison — R² (Coefficient of Determination): Higher is better")
ax.set_xlabel("Models")
ax.set_ylabel("R²")
ax.set_ylim(0, 1)  # remove or adjust if your R² can be negative
fig.tight_layout()
plt.show()

# Bars for RMSE & MAE (left axis) and a line for R² (right axis)
fig, ax1 = plt.subplots()
x = np.arange(len(results_df))
ax1.bar(x - 0.2, results_df["RMSE"].to_numpy(), width=0.4, label="RMSE")
ax1.bar(x + 0.2, results_df["MAE"].to_numpy(), width=0.4, label="MAE")
ax1.set_xticks(x)
ax1.set_xticklabels(results_df["Model"].tolist(), rotation=25, ha="right")
ax1.set_xlabel("Models")
ax1.set_ylabel("Error (RMSE/MAE)")
ax1.legend(loc="upper left")

ax2 = ax1.twinx()
ax2.plot(x, results_df["R2"].to_numpy(), marker="o", linestyle="-", label="R²", color='black')
ax2.set_ylabel("R²")
ax2.set_ylim(0, 1)

plt.title("Model Metrics Overview: Errors (lower better) and R² (higher better)")
fig.tight_layout()
plt.show()

"""Save the models"""

# Save trained models + metadata from this notebook
import json
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

SAVE_DIR = Path("./models")   # stays next to your notebook
SAVE_DIR.mkdir(parents=True, exist_ok=True)

def _safe(name: str) -> str:
    return name.lower().replace(" ", "_")

def _infer_feature_names(X):
    if isinstance(X, pd.DataFrame):
        return X.columns.tolist()
    X = np.asarray(X)
    return [f"f{i}" for i in range(X.shape[1])]

def _metrics(y_true, y_pred):
    return {
        "rmse": float(np.sqrt(mean_squared_error(y_true, y_pred))),
        "mae": float(mean_absolute_error(y_true, y_pred)),
        "r2": float(r2_score(y_true, y_pred)),
    }

def save_models(
    models: dict,
    X_train,
    y_train,
    X_test,
    y_test,
    target_name: str = "popularity",
):
    """
    Saves each fitted model in `models` as .joblib and writes meta + registry.json.
    If a model is not yet fitted, it will be fitted on (X_train, y_train) here.
    """
    # Ensure DataFrames for consistent feature names
    feature_names = _infer_feature_names(X_train)
    Xtr = pd.DataFrame(np.asarray(X_train), columns=feature_names) if not isinstance(X_train, pd.DataFrame) else X_train.copy()
    Xte = pd.DataFrame(np.asarray(X_test),  columns=feature_names) if not isinstance(X_test,  pd.DataFrame) else X_test.copy()

    registry = []

    for name, model in models.items():
        # Fit if needed (best effort)
        needs_fit = False
        try:
            _ = model.predict(Xtr.iloc[:1] if isinstance(Xtr, pd.DataFrame) else np.asarray(Xtr)[:1])
        except Exception:
            needs_fit = True
        if needs_fit:
            model.fit(Xtr, y_train)

        # Evaluate on test for metadata
        preds = model.predict(Xte)
        m = _metrics(y_test, preds)

        # Persist model
        fname = f"{_safe(name)}.joblib"
        fpath = SAVE_DIR / fname
        joblib.dump(model, fpath)

        # Per-model metadata
        meta = {
            "model_name": name,
            "file": fname,
            "framework": "scikit-learn",
            "type": "regression",
            "features": feature_names,
            "target": target_name,
            "metrics": m,
        }
        (SAVE_DIR / f"{_safe(name)}.meta.json").write_text(json.dumps(meta, indent=2), encoding="utf-8")

        registry.append(meta)
        print(f"Saved {name}  ->  {fpath}")

    # Combined registry
    (SAVE_DIR / "registry.json").write_text(json.dumps(registry, indent=2), encoding="utf-8")
    print("\nWrote registry.json with", len(registry), "entries to", SAVE_DIR.as_posix())

# Assumes you already defined: models, X_train, X_test, y_train, y_test
save_models(models, X_train, y_train, X_test, y_test, target_name="popularity")

"""Quick Check"""

from glob import glob
glob("./models/*")

